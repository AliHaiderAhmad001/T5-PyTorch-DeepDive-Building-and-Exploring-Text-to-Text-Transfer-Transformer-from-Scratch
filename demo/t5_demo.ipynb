{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pPl6HPCDWiRr",
        "BB6U6IVIbJzS",
        "9NoIDYl2bca-",
        "-zMFnuPRDrrC",
        "dp-yX9wLhpHi",
        "dtC3njsyhVcS",
        "z6zKHdYvzbU3",
        "gLMijPnbhAuL",
        "-gFOxK9yhy5C",
        "BeH_0GTMoKAf",
        "ULRZ8O3_otbS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**"
      ],
      "metadata": {
        "id": "ssB7kFR9Wdwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code"
      ],
      "metadata": {
        "id": "AhwpqEj3WruK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note:** The goal of this project is to provide a deep understanding of the GPT architecture and its inner workings. So, it's only for educational purposes."
      ],
      "metadata": {
        "id": "o-Q6JL9J1FXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**"
      ],
      "metadata": {
        "id": "pPl6HPCDWiRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The T5 language model, short for \"Text-to-Text Transfer Transformer\", is a type of deep learning model designed for a wide range of natural language processing (NLP) tasks. It was developed by researchers at Google and introduced in a paper titled \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" in 2019. The unique aspect of T5 is its text-to-text framework, where every NLP task is reframed as a text generation problem. This means inputs are always text strings, and the model is trained to produce outputs in the form of text strings as well.\n",
        "\n",
        "T5 covers a variety of tasks including translation, summarization, question answering, and classification, by simply changing the format of the input text. For example, to perform translation from English to French, the input could be prepended with \"Translate English to French:\", followed by the text to be translated. For summarization, the input could start with \"Summarize:\", followed by the text to summarize.\n",
        "\n",
        "The T5 model was pre-trained on a large corpus called the \"Colossal Clean Crawled Corpus\" (C4), which is a cleaned version of Common Crawl data. The pre-training involved a denoising objective similar to the one used by BERT (another prominent NLP model), but with a focus on generating the entire input sequence rather than predicting individual masked words.\n",
        "\n",
        "T5 comes in various sizes such as small, base and large, with the number of parameters ranging from hundreds of millions to billions, allowing it to be adaptable for different computational needs and tasks. Its architecture is based on the Transformer model, which relies on self-attention mechanisms to process sequences of text.\n",
        "\n",
        "**Tips:**\n",
        "Before you complete this demo, I assume that you have a good understanding of the transformer architecture. If not, I advise you to start with:\n",
        "* (Required) [Transformer-from-Scratch-with-Tensorflow](https://github.com/AliHaiderAhmad001/Transformer-from-Scratch-with-Tensorflow).\n",
        "* (Optional) [BERT-from-Scratch-with-PyTorch](https://github.com/AliHaiderAhmad001/BERT-from-Scratch-with-PyTorch).\n",
        "* (Optional) [GPT-from-Scratch-with-Tensorflow](https://github.com/AliHaiderAhmad001/GPT-from-Scratch-with-Tensorflow).\n",
        "\n",
        "I'm also assuming you've read [the paper](https://arxiv.org/abs/1910.10683) or at least the part related to the Training Objective and Model.\n",
        "\n",
        "This will make life clearer and easier for you.\n",
        "\n",
        "\n",
        "**Some considerations related to the differences from orginal Transformer architecture**\n",
        "\n",
        "**Note:** We must know that the transformer model is a model that was created primarily for the task of translation and not as a language model or learning transfer model. So here we are not comparing them, as they are two different things, but we are talking about the structure of the model, which is comparable.\n",
        "\n",
        "Differences in the implementation of the Transformer architecture compared to the original proposal by Vaswani et al. (2017). Here are the identified differences:\n",
        "\n",
        "1. **Layer Normalization.** The original Transformer proposed by Vaswani et al. (2017) uses a form of layer normalization that includes both rescaling and additive bias. However, in the described implementation, a simplified version of layer normalization is employed, where only rescaling is applied, and no additive bias is used. Additionally, the layer normalization is placed outside the residual path.\n",
        "\n",
        "2. **Position Embedding Scheme.** While the original Transformer used sinusoidal position signals or learned position embeddings, the described implementation uses a different approach. It employs relative position embeddings instead of fixed embeddings for each position. The relative position embeddings produce a different learned embedding based on the offset between the \"key\" and \"query\" in the self-attention mechanism. This scheme involves adding a scalar to the corresponding logit used for computing attention weights. I'm not going to explain it, just explain it and talk about how to implement it [here](https://github.com/AliHaiderAhmad001/T5-Relative-Position).\n",
        "3. **Training Objective.** A new training objective derived from Bert's MLM is used (This point relates to the difference between it and other language models and learning transfer models whose structure is based on the transformer model.).\n",
        "4. Other minor differences, such as weights are shared between the embedding layer and the final output layer."
      ],
      "metadata": {
        "id": "V6pkMGadIeiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset & Download**"
      ],
      "metadata": {
        "id": "BB6U6IVIbJzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original model uses a special data set called [C4](https://www.tensorflow.org/datasets/catalog/c4). It's so huge we can't handle it here. So as usual we will grab a simple dataset like IMDB and apply the general approach to it."
      ],
      "metadata": {
        "id": "QIhboI28bmLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/projects/T5-PyTorch-DeepDive-Building-and-Exploring-Text-to-Text-Transfer-Transformer-from-Scratch/datasets'\n",
        "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "#!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "yff8OQ6YbLDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be2fee8-f988-4271-ef98-e6829fa58572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/projects/T5-PyTorch-DeepDive-Building-and-Exploring-Text-to-Text-Transfer-Transformer-from-Scratch/datasets'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "9NoIDYl2bca-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/projects/T5-PyTorch-DeepDive-Building-and-Exploring-Text-to-Text-Transfer-Transformer-from-Scratch/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NFEXszebaWf",
        "outputId": "338d3db5-6033-448e-8712-e45a1f48130a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/projects/T5-PyTorch-DeepDive-Building-and-Exploring-Text-to-Text-Transfer-Transformer-from-Scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "# pip install sentencepiece\n",
        "#!pip install tqdm"
      ],
      "metadata": {
        "id": "CsjIEUvJhwbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2741653f-0799-4bb8-dfd0-ebf2e6040ebe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "-zMFnuPRDrrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random as rd\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from transformers import T5TokenizerFast\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Tuple, Dict, Union, Any, Callable, Iterable"
      ],
      "metadata": {
        "id": "u6l7lQERDtUP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08"
      ],
      "metadata": {
        "id": "05fNxorY87Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data preprocessing**"
      ],
      "metadata": {
        "id": "dp-yX9wLhpHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Text.data"
      ],
      "metadata": {
        "id": "_8VdGNM6sBLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset consists of two folders train and test, each of which includes two subfolders named pos and neg containing a specified number of text documents representing movie reviews. This does not matter to us, what matters to us is that they are text documents that we can train the model on. I will collect these texts into one file named \"text_data.txt\".\n"
      ],
      "metadata": {
        "id": "j9y92oqBsG9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(f_path):\n",
        "    \"\"\"Reads and returns the content of a file.\"\"\"\n",
        "    try:\n",
        "        with open(f_path, \"r\", encoding=\"utf-8\") as file:  # Added encoding\n",
        "            doc = file.read().strip()\n",
        "    except Exception as e:  # Added error handling\n",
        "        print(f\"Error reading {f_path}: {e}\")\n",
        "        doc = \"\"\n",
        "    return doc\n",
        "\n",
        "def get_filenames(data_dir):\n",
        "    filenames = []\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        if os.path.basename(root) in ['neg', 'pos'] and os.path.basename(os.path.dirname(root)) in ['train', 'test']:\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                filenames.append(filepath)\n",
        "    return filenames\n",
        "\n",
        "def create_data_file(data_dir, f_target='dataset/text_data.txt'):\n",
        "    \"\"\"Combines text from all files in a directory into a single file.\"\"\"\n",
        "    files = get_filenames(data_dir)\n",
        "    # Ensure the target directory exists\n",
        "    os.makedirs(os.path.dirname(f_target), exist_ok=True)  # Ensure target directory exists\n",
        "    with open(f_target, \"w\", encoding=\"utf-8\") as f:  # Added encoding\n",
        "        for f_path in tqdm(files, desc=\"Processing files\"):\n",
        "            doc = read_file(f_path)\n",
        "            f.write(doc + \"\\n\")\n",
        "\n",
        "#create_data_file('dataset/aclImdb')"
      ],
      "metadata": {
        "id": "MzTUTnDVVE0w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training File"
      ],
      "metadata": {
        "id": "qa49TtD9sNd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now create a function to perform the corruption task (or masking).\n",
        "\n",
        "Given the sentence:\n",
        "\n",
        "> Thank you for inviting me to your party last week.\n",
        "\n",
        "**Input**: `Thank you <X> me to your party <Y> week`\n",
        "**Target**: `<X> for inviting <Y> last <Z>`\n",
        "\n",
        "> We process the sentence `Thank you for inviting me to your party last week.` The words `for`, `inviting` and `last` are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as `<X>` and `<Y>`) that is unique over the example. Since `for` and `inviting` occur consecutively, they are replaced by a single sentinel `<X>`. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token `<Z>`."
      ],
      "metadata": {
        "id": "Oradr--ByEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_sentence(tokens: List[str], prop: float) -> Tuple[List[str], List[str]]:\n",
        "    def get_next(index, li):\n",
        "        return li[index] if index < len(li) else -1\n",
        "\n",
        "    def get_sentinel():\n",
        "        nonlocal sentinel_count\n",
        "        sentinel_token = f\"<extra_id_{sentinel_count}>\"\n",
        "        sentinel_count += 1\n",
        "        return sentinel_token\n",
        "\n",
        "    # Calculate the number of tokens to mask, and randomly select indices for masking\n",
        "    num_masked = ceil(prop * len(tokens))\n",
        "    masked_indices = sorted(rd.sample(range(len(tokens)), num_masked))\n",
        "    #print(masked_indices)\n",
        "    input_sequence = tokens.copy()\n",
        "    target_sequence = []\n",
        "    sentinel_count = 0\n",
        "\n",
        "    i = 0\n",
        "    while i < len(masked_indices):\n",
        "        idx = masked_indices[i]\n",
        "        sentinel_token = get_sentinel()\n",
        "\n",
        "        # For the first token in a consecutive sequence, replace with sentinel\n",
        "        if i == 0 or masked_indices[i-1] != idx - 1:\n",
        "            input_sequence[idx] = sentinel_token\n",
        "            target_sequence.append(sentinel_token)\n",
        "\n",
        "        target_sequence.append(tokens[idx])\n",
        "\n",
        "        # If the next index is consecutive, we remove it from input_sequence\n",
        "        # by marking it as None; we'll filter these out later\n",
        "        next_idx = get_next(i+1, masked_indices)\n",
        "        while idx + 1 == next_idx:\n",
        "            i += 1\n",
        "            idx = masked_indices[i]\n",
        "            input_sequence[idx] = None  # Mark for removal\n",
        "            target_sequence.append(tokens[idx])\n",
        "            next_idx = get_next(i+1, masked_indices)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Remove marked indices (None) from input_sequence\n",
        "    input_sequence = [token for token in input_sequence if token is not None]\n",
        "    return (input_sequence, target_sequence)"
      ],
      "metadata": {
        "id": "ZZubfiROhty1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "tokens = [\"Younes\", \"and\", \"Lukasz\", \"were\", \"working\", \"together\", \"in\", \"the\", \"lab\", \"yesterday\", \"after\", \"lunch.\"]\n",
        "input_sequence, target_sequence = mask_sentence(tokens, prop=0.50)\n",
        "\n",
        "print(\"Input Sequence:\", \" \".join(tokens))\n",
        "print(\"Input Sequence:\", \" \".join(input_sequence))\n",
        "print(\"Target Sequence:\", \" \".join(target_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psdurIaDhv30",
        "outputId": "196c3d76-8810-4ff3-f323-7b58bf599fa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: Younes and Lukasz were working together in the lab yesterday after lunch.\n",
            "Input Sequence: Younes and <extra_id_0> working <extra_id_1> in the <extra_id_2> yesterday <extra_id_3>\n",
            "Target Sequence: <extra_id_0> Lukasz were <extra_id_1> together <extra_id_2> lab <extra_id_3> after lunch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create the `preprocessing` function that processes the texts and stores them in a file called \"train.txt\". The processing process includes tokenizing the texts, the process of corruption, and creating the inputs and outputs of the texts, but this stage will not include converting the tokens into identifiers. This file we will use for the training process."
      ],
      "metadata": {
        "id": "NBfTT8u9yc8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_std(line, lower_case=True):\n",
        "    \"\"\" Remove HTML line-break tags and lowercase the sentence.\"\"\"\n",
        "    line = re.sub(\"<br />\", \" \", line).strip()\n",
        "    if lower_case:\n",
        "        line = line.lower()\n",
        "    return line\n",
        "\n",
        "def preprocessing(f_path, f_target, tokenizer_path='t5-base', prop = 0.15, max_words=768, extra_ids = 115, lower_case = True):\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(tokenizer_path, extra_ids = extra_ids+1)\n",
        "    with open(f_target, 'w') as outfile:\n",
        "          with open(f_path, 'r') as infile:\n",
        "              for line in infile:\n",
        "                    line = custom_std(line, lower_case = lower_case)\n",
        "                    tokens = tokenizer.tokenize(line) # list[str]\n",
        "                    # Ensure the sentence is within the max_words limit\n",
        "                    tokens = tokens[:max_words]\n",
        "                    encoder_in, target_str = mask_sentence(tokens, prop)\n",
        "                    decoder_in = target_str.copy()\n",
        "                    decoder_out = target_str.copy()[1:]\n",
        "                    decoder_out.append(f\"<extra_id_{extra_ids+1}>\")\n",
        "                    outfile.write(\" \".join(encoder_in) +'\\n' + \" \".join(decoder_in) + '\\n' + \" \".join(decoder_out) + '\\n\\n')"
      ],
      "metadata": {
        "id": "hYaErv8OkXFa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "#preprocessing('dataset/dummy_data/sample_test.txt', 'dataset/dummy_data/out.txt')"
      ],
      "metadata": {
        "id": "8hES1rwBJY48"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing('dataset/text_data.txt', 'dataset/train.txt')"
      ],
      "metadata": {
        "id": "Q0i1VP3mrpYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Config**"
      ],
      "metadata": {
        "id": "dtC3njsyhVcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"\n",
        "    Configuration class for your model and training.\n",
        "\n",
        "    Args:\n",
        "        prop (float): Proportion of tokens to mask in each sentence. Default is 0.15.\n",
        "        tokenizer_path (str): Path to the tokenizer. Default is \"bert-base-uncased\".\n",
        "        max_token_len (int): Maximum sequence length. Default is 128.\n",
        "        data_dir (str): Directory containing the data file.\n",
        "\n",
        "        # Embeddings params\n",
        "        hidden_size (int): Size of the hidden layers. Default is 768.\n",
        "        vocab_size (int): Size of the vocabulary. Default is 30522.\n",
        "        hidden_dropout_prob (float): Dropout probability for hidden layers. Default is 0.1.\n",
        "\n",
        "        # Attention params\n",
        "        num_heads (int): Number of attention heads. Default is 8.\n",
        "\n",
        "        # model params\n",
        "        num_blocks (int): Number of blocks in the BERT model. Default is 12.\n",
        "\n",
        "        # Optimizer params\n",
        "        n_warmup_steps (int): Number of warmup steps for the optimizer. Default is 10000.\n",
        "        lr (float): Learning rate for the optimizer. Default is 1e-2.\n",
        "        betas (tuple): Betas for the optimizer. Default is (0.9, 0.999).\n",
        "\n",
        "        # Trainer params\n",
        "        cuda_devices (list): List of CUDA devices. Default is None.\n",
        "        with_cuda (bool): Flag to use CUDA. Default is True.\n",
        "        log_freq (int): Logging frequency. Default is 10.\n",
        "        batch_size (int): Batch size for training. Default is 64.\n",
        "        save_path (str): Path to save model checkpoints. Default is 'tmp/checkpoints'.\n",
        "\n",
        "        # Run the model params\n",
        "        seed (int): Random seed for reproducibility. Default is 0.\n",
        "        test_dataset (str): Path to the test dataset or None. Default is None.\n",
        "        epochs (int): Number of training epochs. Default is 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prop=0.15, tokenizer_path=\"t5-base\", max_token_len=768, data_dir=\"dataset/train.txt\",\n",
        "                 hidden_size=768, bidirectional = True, num_buckets = 32, max_distance = 128,\n",
        "                 vocab_size=32000, hidden_dropout_prob=0.1, num_heads=8, num_blocks=12,\n",
        "                 n_warmup_steps=10000, lr=1e-2, betas=(0.9, 0.999),\n",
        "                 cuda_devices=None, with_cuda=True, log_freq=10, batch_size=64, save_path='tmp/checkpoints',\n",
        "                 seed=0, test_dataset=None, epochs=1):\n",
        "\n",
        "        # Dataset params\n",
        "        self.prop = prop\n",
        "        self.tokenizer_path = tokenizer_path\n",
        "        self.max_token_len = max_token_len\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Relative pos params\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "\n",
        "        # Embeddings params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size + 100 # We add 100 for the number of sentinals we use\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "\n",
        "        # Attention params\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # BERT model params\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Optimizer params\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "\n",
        "        # Trainer params\n",
        "        self.cuda_devices = cuda_devices\n",
        "        self.with_cuda = with_cuda\n",
        "        self.log_freq = log_freq\n",
        "        self.batch_size = batch_size\n",
        "        self.save_path = save_path\n",
        "\n",
        "        # Run the model params\n",
        "        self.seed = seed\n",
        "        self.test_dataset = test_dataset\n",
        "        self.epochs = epochs\n"
      ],
      "metadata": {
        "id": "Mp3QlqXQFa-n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataLoader**"
      ],
      "metadata": {
        "id": "z6zKHdYvzbU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset class is a starting point and might need adjustments based on specific task requirements, such as handling different data formats or more complex preprocessing steps.\n",
        "\n",
        "- **Efficiency**: For large datasets, consider loading and processing data in more memory-efficient ways, especially when dealing with very large files.\n",
        "  \n",
        "- **Error Handling**: Consider adding error handling for file reading and processing to ensure robustness, especially with varied or inconsistent data formats.\n",
        "\n",
        "- **Batch Processing**: When used with `DataLoader`, this dataset setup allows for efficient batch processing of data, including automatic batching, shuffling, and parallel data loading.\n"
      ],
      "metadata": {
        "id": "5dNRUaSE6nyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import T5TokenizerFast\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for handling custom text data.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the file containing the text data.\n",
        "        tokenizer (str or transformers.PreTrainedTokenizerFast, optional): Pre-trained tokenizer or tokenizer name to use for tokenization. Defaults to 't5-base'.\n",
        "        max_token_len (int, optional): Maximum length of tokens for encoding. Defaults to 512.\n",
        "    \"\"\"\n",
        "    def __init__(self, filename, tokenizer='t5-base', max_token_len=512):\n",
        "        self.tokenizer = T5TokenizerFast.from_pretrained(tokenizer)\n",
        "        self.items = []\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            while True:\n",
        "                encoder_in = file.readline().strip()\n",
        "                if not encoder_in:\n",
        "                    break\n",
        "                decoder_in = file.readline().strip()\n",
        "                decoder_out = file.readline().strip()\n",
        "                self.items.append((encoder_in, decoder_in, decoder_out))\n",
        "\n",
        "                _ = file.readline()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset by index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the encoder and decoder inputs, attention mask, and labels.\n",
        "        \"\"\"\n",
        "        encoder_in, decoder_in, decoder_out = self.items[idx]\n",
        "\n",
        "        encoder_ids = self.tokenizer(\n",
        "            encoder_in,\n",
        "            max_length=self.max_token_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_ids = self.tokenizer(\n",
        "            decoder_in,\n",
        "            max_length=self.max_token_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "        labels = self.tokenizer(\n",
        "            decoder_out,\n",
        "            max_length=self.max_token_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        return {\n",
        "            \"encoder_ids\": encoder_ids[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoder_ids[\"attention_mask\"].squeeze(),\n",
        "            \"decoder_ids\": target_ids.squeeze(),\n",
        "            \"labels\": labels.squeeze()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ADfWYwyo6D2p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "dataset_path = \"dataset/dummy_data/out.txt\"\n",
        "custom_dataset = CustomTextDataset(filename=dataset_path, max_token_len=10)\n",
        "\n",
        "data_loader = DataLoader(custom_dataset, batch_size=2)\n",
        "\n",
        "for batch in data_loader:\n",
        "    encoder_ids = batch['encoder_ids']\n",
        "    decoder_ids = batch['decoder_ids']\n",
        "    labels = batch['labels']\n",
        "    att_mask = batch['attention_mask']\n",
        "    print(encoder_ids.shape, decoder_ids.shape, labels.shape)\n",
        "    print(\"------------\")\n",
        "    assert encoder_ids.shape == labels.shape\n",
        "    print(encoder_ids)\n",
        "    print(decoder_ids)\n",
        "    print(labels)\n",
        "    print(att_mask)\n",
        "    print('----------------------------')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrBUiPDvK2ZJ",
        "outputId": "48ac6aeb-5020-4a2d-e166-8c0ae9498ac0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10]) torch.Size([2, 10]) torch.Size([2, 10])\n",
            "------------\n",
            "tensor([[    3,  5627, 32099,     3,     9,  8434,  2870,     3,    49,     1],\n",
            "        [  168,     3,     6,    34,     3,    31,     3,     7, 32099,     1]])\n",
            "tensor([[32099,    38, 32098,    11, 32097,     3,     6, 32096,     3,     1],\n",
            "        [32099,  1346,    12, 32098,     3,    55,     1,     0,     0,     0]])\n",
            "tensor([[   38, 32098,    11, 32097,     3,     6, 32096,     3,     5,     1],\n",
            "        [ 1346,    12, 32098,     3,    55,     3,     2, 25666,   834,     1]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqj_XZSMSzJO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **T5 Bulding Blocks**"
      ],
      "metadata": {
        "id": "gLMijPnbhAuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the components are not much different from the original transformer model that we explained previously. We have already mentioned the basic changes."
      ],
      "metadata": {
        "id": "aI2PwxXe84iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RelativePositionBias"
      ],
      "metadata": {
        "id": "93aq3otWhXB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've explained it [here](https://github.com/AliHaiderAhmad001/T5-Relative-Position)."
      ],
      "metadata": {
        "id": "ig72hpkw66Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RelativePositionBias(nn.Module):\n",
        "    \"\"\"\n",
        "    Translate relative position to a bucket number for relative attention.\n",
        "\n",
        "    The relative position is defined as memory_position - query_position, i.e.\n",
        "    the distance in tokens from the attending position to the attended-to\n",
        "    position. If bidirectional=False, then positive relative positions are\n",
        "    invalid.\n",
        "\n",
        "    We use smaller buckets for small absolute relative_position and larger buckets\n",
        "    for larger absolute relative_positions. All relative positions >=max_distance\n",
        "    map to the same bucket. All relative positions <=-max_distance map to the\n",
        "    same bucket. This should allow for more graceful generalization to longer\n",
        "    sequences than the model has been trained on.\n",
        "\n",
        "    Args:\n",
        "        bidirectional (bool): Whether the attention is bidirectional.\n",
        "        num_buckets (int): Number of buckets.\n",
        "        max_distance (int): Maximum distance for relative positions.\n",
        "        num_heads (int): Number of attention heads.\n",
        "\n",
        "    # REFRANCE: https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(RelativePositionBias, self).__init__()\n",
        "        self.bidirectional = config.bidirectional\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.num_heads = config.num_heads\n",
        "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.num_heads)\n",
        "\n",
        "    @staticmethod\n",
        "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
        "        \"\"\"\n",
        "        Translate relative position to a bucket number.\n",
        "\n",
        "        Args:\n",
        "            relative_position (torch.Tensor): Relative position tensor.\n",
        "            bidirectional (bool): Whether the attention is bidirectional.\n",
        "            num_buckets (int): Number of buckets.\n",
        "            max_distance (int): Maximum distance for relative positions.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Bucket number tensor.\n",
        "        \"\"\"\n",
        "        ret = 0 * relative_position  # Initialized to zero to handle both positive and negative positions\n",
        "        if bidirectional:\n",
        "            num_buckets //= 2  # Halve the buckets for bidirectional case\n",
        "            ret += (relative_position < 0).long() * num_buckets\n",
        "            relative_position = relative_position.abs()\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_position < max_exact\n",
        "\n",
        "        # Compute val_if_large with safe clamping within [0, num_buckets - 1]\n",
        "        val_if_large = max_exact + (\n",
        "            torch.log(relative_position.float() / max_exact) /\n",
        "            torch.log(torch.tensor(max_distance / max_exact, dtype=torch.float)) *\n",
        "            (num_buckets - max_exact)\n",
        "        ).long()\n",
        "        val_if_large = torch.minimum(val_if_large, torch.tensor(num_buckets - 1, dtype=torch.long))\n",
        "\n",
        "        # Combine small and large relative positions\n",
        "        ret += torch.where(is_small, relative_position, val_if_large)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def compute_bias(self, qlen, klen):\n",
        "        \"\"\"\n",
        "        Compute binned relative position bias.\n",
        "\n",
        "        Args:\n",
        "            qlen (int): Length of the query sequence.\n",
        "            klen (int): Length of the key sequence.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Relative position bias tensor.\n",
        "        \"\"\"\n",
        "        device = self.relative_attention_bias.weight.device\n",
        "        context_position = torch.arange(qlen, dtype=torch.long, device=device)[:, None]\n",
        "        memory_position = torch.arange(klen, dtype=torch.long, device=device)[None, :]\n",
        "        relative_position = memory_position - context_position\n",
        "\n",
        "        rp_bucket = self._relative_position_bucket(\n",
        "            relative_position,\n",
        "            bidirectional=self.bidirectional,\n",
        "            num_buckets=self.num_buckets,\n",
        "            max_distance=self.max_distance\n",
        "        )\n",
        "\n",
        "        values = self.relative_attention_bias(rp_bucket)\n",
        "        values = values.permute([2, 0, 1]).unsqueeze(0)\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "    def forward(self, qlen, klen):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            qlen (int): Length of the query sequence.\n",
        "            klen (int): Length of the key sequence.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Relative position bias tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.compute_bias(qlen, klen)\n"
      ],
      "metadata": {
        "id": "87OCiEkiPc3r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Configx:\n",
        "    bidirectional = True\n",
        "    num_buckets = 32\n",
        "    max_distance = 128\n",
        "    num_heads = 8\n",
        "\n",
        "# Example usage\n",
        "config = Configx()\n",
        "relative_position_bias_module = RelativePositionBias(config)\n",
        "qlen, klen = 768, 768\n",
        "bias_tensor = relative_position_bias_module.forward(qlen, klen)\n",
        "print(bias_tensor.shape)\n",
        "print(bias_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0qCcRpoaWV6",
        "outputId": "dc5add15-15a4-4e91-ef8c-9ca87e3fb1b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768, 768])\n",
            "tensor([[[[-0.8781,  0.4954, -1.0465,  ...,  0.8665,  0.8665,  0.8665],\n",
            "          [ 0.5903, -0.8781,  0.4954,  ...,  0.8665,  0.8665,  0.8665],\n",
            "          [ 1.0667,  0.5903, -0.8781,  ...,  0.8665,  0.8665,  0.8665],\n",
            "          ...,\n",
            "          [-0.0386, -0.0386, -0.0386,  ..., -0.8781,  0.4954, -1.0465],\n",
            "          [-0.0386, -0.0386, -0.0386,  ...,  0.5903, -0.8781,  0.4954],\n",
            "          [-0.0386, -0.0386, -0.0386,  ...,  1.0667,  0.5903, -0.8781]],\n",
            "\n",
            "         [[ 0.6085, -0.3646,  0.0968,  ..., -0.4195, -0.4195, -0.4195],\n",
            "          [ 0.6747,  0.6085, -0.3646,  ..., -0.4195, -0.4195, -0.4195],\n",
            "          [ 1.2394,  0.6747,  0.6085,  ..., -0.4195, -0.4195, -0.4195],\n",
            "          ...,\n",
            "          [ 0.5440,  0.5440,  0.5440,  ...,  0.6085, -0.3646,  0.0968],\n",
            "          [ 0.5440,  0.5440,  0.5440,  ...,  0.6747,  0.6085, -0.3646],\n",
            "          [ 0.5440,  0.5440,  0.5440,  ...,  1.2394,  0.6747,  0.6085]],\n",
            "\n",
            "         [[ 1.2230,  1.2487,  0.2485,  ..., -1.3623, -1.3623, -1.3623],\n",
            "          [ 0.1266,  1.2230,  1.2487,  ..., -1.3623, -1.3623, -1.3623],\n",
            "          [-0.1837,  0.1266,  1.2230,  ..., -1.3623, -1.3623, -1.3623],\n",
            "          ...,\n",
            "          [ 1.5688,  1.5688,  1.5688,  ...,  1.2230,  1.2487,  0.2485],\n",
            "          [ 1.5688,  1.5688,  1.5688,  ...,  0.1266,  1.2230,  1.2487],\n",
            "          [ 1.5688,  1.5688,  1.5688,  ..., -0.1837,  0.1266,  1.2230]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.9474,  1.2916, -0.3282,  ...,  0.4560,  0.4560,  0.4560],\n",
            "          [-0.1073,  0.9474,  1.2916,  ...,  0.4560,  0.4560,  0.4560],\n",
            "          [-1.7708, -0.1073,  0.9474,  ...,  0.4560,  0.4560,  0.4560],\n",
            "          ...,\n",
            "          [-1.0147, -1.0147, -1.0147,  ...,  0.9474,  1.2916, -0.3282],\n",
            "          [-1.0147, -1.0147, -1.0147,  ..., -0.1073,  0.9474,  1.2916],\n",
            "          [-1.0147, -1.0147, -1.0147,  ..., -1.7708, -0.1073,  0.9474]],\n",
            "\n",
            "         [[ 0.4366, -0.3988, -1.4307,  ...,  0.1735,  0.1735,  0.1735],\n",
            "          [-0.0645,  0.4366, -0.3988,  ...,  0.1735,  0.1735,  0.1735],\n",
            "          [-1.1155, -0.0645,  0.4366,  ...,  0.1735,  0.1735,  0.1735],\n",
            "          ...,\n",
            "          [-0.0100, -0.0100, -0.0100,  ...,  0.4366, -0.3988, -1.4307],\n",
            "          [-0.0100, -0.0100, -0.0100,  ..., -0.0645,  0.4366, -0.3988],\n",
            "          [-0.0100, -0.0100, -0.0100,  ..., -1.1155, -0.0645,  0.4366]],\n",
            "\n",
            "         [[ 0.9132, -0.2347,  0.2682,  ...,  0.7527,  0.7527,  0.7527],\n",
            "          [-0.8902,  0.9132, -0.2347,  ...,  0.7527,  0.7527,  0.7527],\n",
            "          [-0.5659, -0.8902,  0.9132,  ...,  0.7527,  0.7527,  0.7527],\n",
            "          ...,\n",
            "          [-0.1579, -0.1579, -0.1579,  ...,  0.9132, -0.2347,  0.2682],\n",
            "          [-0.1579, -0.1579, -0.1579,  ..., -0.8902,  0.9132, -0.2347],\n",
            "          [-0.1579, -0.1579, -0.1579,  ..., -0.5659, -0.8902,  0.9132]]]],\n",
            "       grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings layer"
      ],
      "metadata": {
        "id": "yHM-aFu6hbL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from relative_position_bias import RelativePositionBias\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    The Embeddings class is a crucial component of transformer-based models, responsible for converting input token IDs into dense vector representations. These embeddings are the first step in processing input data, transforming discrete token IDs into continuous vectors that can encapsulate semantic information. This class not only performs token embedding lookup but also applies layer normalization and dropout for regularization.\n",
        "\n",
        "    The embedding vectors produced by this class serve as the input to subsequent layers of the model, facilitating the learning of token-specific features in the context of the task at hand, whether it be language understanding, translation, or another natural language processing (NLP) application.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_size (int): The size of the embedding vectors. This is also the size of the hidden layers throughout the model.\n",
        "        vocab_size (int): The size of the vocabulary, determining the number of unique token embeddings the layer can produce.\n",
        "        hidden_dropout_prob (float): The dropout probability, used to randomly zero elements of the embedding vectors with this probability to prevent overfitting.\n",
        "        token_embeddings (nn.Embedding): The PyTorch embedding layer that maps token IDs to embedding vectors.\n",
        "        dropout (nn.Dropout): The dropout layer applied to the embedding vectors.\n",
        "        norm (nn.LayerNorm): The layer normalization applied to the embedding vectors.\n",
        "\n",
        "    Methods:\n",
        "        forward(input_ids: torch.Tensor) -> torch.Tensor\n",
        "            Performs the embedding lookup, followed by layer normalization and dropout, to produce the final embedding vectors for the input tokens.\n",
        "\n",
        "    Args:\n",
        "        config (object): A configuration object containing the attributes necessary to initialize the embedding layer. These attributes include:\n",
        "            - hidden_size: The dimensionality of the embedding vectors and the model's hidden layers.\n",
        "            - vocab_size: The total size of the vocabulary.\n",
        "            - hidden_dropout_prob: The dropout probability for regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.hidden_dropout_prob = config.hidden_dropout_prob\n",
        "\n",
        "        # Initialize the embedding layer, dropout, and layer normalization\n",
        "        self.token_embeddings = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
        "        self.norm = nn.LayerNorm(self.hidden_size, eps=1e-6)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Processes input token IDs through the embedding layer, followed by layer normalization and dropout.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor containing a batch of input token IDs.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The resulting tensor after applying embeddings, layer normalization, and dropout. This tensor is ready to be fed into subsequent layers of the model.\n",
        "        \"\"\"\n",
        "        x = self.token_embeddings(input_ids)\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "E4fUALTEhid2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attination Head"
      ],
      "metadata": {
        "id": "CtfiEWQChqkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Relation-aware attention head implementation.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        head_dim (int): Dimensionality of the attention head.\n",
        "\n",
        "    Attributes:\n",
        "        query_weights (nn.Linear): Linear layer for query projection.\n",
        "        key_weights (nn.Linear): Linear layer for key projection.\n",
        "        value_weights (nn.Linear): Linear layer for value projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, head_dim, hidden_dropout_prob=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the AttentionHead.\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "            head_dim (int): Dimensionality of the attention head.\n",
        "            hidden_dropout_prob(float)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "        self.query_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.key_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.value_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                 relative_biases:torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor.\n",
        "            key (torch.Tensor): Key tensor.\n",
        "            value (torch.Tensor): Value tensor.\n",
        "            mask (torch.Tensor): Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated value embeddings after applying attention mechanism.\n",
        "        \"\"\"\n",
        "        query: torch.Tensor = self.query_weights(query)\n",
        "        key: torch.Tensor = self.key_weights(key)\n",
        "        value: torch.Tensor = self.value_weights(value)\n",
        "\n",
        "        att_scores: torch.Tensor = (torch.matmul(query, key.transpose(1, 2)) + relative_biases) / self.head_dim ** 0.5\n",
        "\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                # Padding mask case: [batch_size, seq_len]\n",
        "                # Unsqueeze to [batch_size, 1, seq_len] to match the broadcasting requirements\n",
        "                mask = mask.unsqueeze(1)\n",
        "            elif mask.dim() == 3:\n",
        "                # Already in [batch_size, seq_len, seq_len] shape, no adjustment needed\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\"Mask dimension is not supported. Must be 2 or 3.\")\n",
        "\n",
        "            # Apply mask - inf where mask == 0, keep original scores where mask != 0\n",
        "            att_scores = att_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        att_weights: torch.Tensor = F.softmax(att_scores, dim=-1)\n",
        "        att_weights: torch.Tensor = self.dropout(att_weights)\n",
        "        n_value: torch.Tensor = torch.matmul(att_weights, value)\n",
        "\n",
        "        return n_value"
      ],
      "metadata": {
        "id": "K5PrsOlUhn20"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention"
      ],
      "metadata": {
        "id": "wlILCBIdhu-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight change here as we have added the concept of relative position encoding"
      ],
      "metadata": {
        "id": "CKWcOemP9Q9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initializes the MultiHeadAttention layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size: int = config.hidden_size\n",
        "        self.num_heads: int = config.num_heads\n",
        "        self.head_dim: int = self.hidden_size // self.num_heads\n",
        "        self.hidden_dropout_prob = config.hidden_dropout_prob\n",
        "        self.attention_heads: nn.ModuleList = nn.ModuleList([AttentionHead(self.hidden_size, self.head_dim, self.hidden_dropout_prob) for head_num in range(self.num_heads)])\n",
        "        self.fc: nn.Linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, relative_position_bias: torch.Tensor,\n",
        "                mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies multi-head attention mechanism to the input query, key, and value tensors.\n",
        "        \"\"\"\n",
        "        attention_outputs: List[torch.Tensor] = [attention_head(query, key, value, mask=mask, relative_biases=relative_position_bias[:,i]) for i, attention_head in enumerate(self.attention_heads)]\n",
        "        hidden_state: torch.Tensor = torch.cat(attention_outputs, dim=-1)\n",
        "        hidden_state: torch.Tensor = self.fc(hidden_state)\n",
        "        return hidden_state"
      ],
      "metadata": {
        "id": "Vir7ctBqhvXy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MockConfig:\n",
        "    def __init__(self):\n",
        "        self.hidden_size = 128\n",
        "        self.vocab_size = 10000\n",
        "        self.hidden_dropout_prob = 0.1\n",
        "        self.num_heads = 8\n",
        "        self.num_buckets = 32\n",
        "        self.max_distance = 128\n",
        "        self.bidirectional = True\n",
        "        # To use later ..\n",
        "        self.max_token_len = 50\n",
        "        self.num_blocks = 2\n",
        "\n",
        "def generate_mask(size, length):\n",
        "    mask = torch.zeros(size, dtype=torch.long)\n",
        "    mask[:, :length] = 1\n",
        "    return mask\n",
        "\n",
        "def test_multi_head_attention():\n",
        "    # Create a mock configuration\n",
        "    config = MockConfig()\n",
        "\n",
        "    multi_head_attention = MultiHeadAttention(config)\n",
        "    relative_position_bias_module = RelativePositionBias(config)\n",
        "\n",
        "    batch_size = 2\n",
        "    seq_length = config.max_token_len\n",
        "    query = key = value = torch.rand(batch_size, seq_length, config.hidden_size)\n",
        "    mask = generate_mask((batch_size, seq_length), 10)\n",
        "    print(mask)\n",
        "    relative_position_bias = relative_position_bias_module(seq_length, seq_length)\n",
        "\n",
        "    # Test the forward pass\n",
        "    output = multi_head_attention(query, key, value, relative_position_bias, mask)\n",
        "\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Output:\", output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_multi_head_attention()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjqcAZQt2gu9",
        "outputId": "62fc9cb0-f6ed-4225-a9f3-cf461f91e119"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0]])\n",
            "Output shape: torch.Size([2, 50, 128])\n",
            "Output: tensor([[[-0.1648, -0.0458, -0.0502,  ...,  0.0264,  0.0408,  0.0373],\n",
            "         [-0.1170, -0.0373, -0.0611,  ..., -0.0275,  0.0291,  0.0716],\n",
            "         [-0.1594, -0.0102, -0.0213,  ...,  0.0250,  0.0551,  0.0406],\n",
            "         ...,\n",
            "         [-0.1604, -0.0366, -0.0443,  ...,  0.0064,  0.0376,  0.0611],\n",
            "         [-0.1654, -0.0004, -0.0515,  ...,  0.0474,  0.0994,  0.0620],\n",
            "         [-0.1212, -0.0330, -0.0424,  ..., -0.0361,  0.0050,  0.0556]],\n",
            "\n",
            "        [[-0.1870, -0.0957,  0.0148,  ...,  0.0259,  0.0489,  0.0511],\n",
            "         [-0.1930, -0.0661,  0.0151,  ...,  0.0125, -0.0037,  0.0297],\n",
            "         [-0.1608, -0.0753, -0.0276,  ...,  0.0009,  0.0398,  0.0679],\n",
            "         ...,\n",
            "         [-0.1522, -0.0832, -0.0041,  ...,  0.0147,  0.0204,  0.1137],\n",
            "         [-0.1693, -0.0657,  0.0046,  ..., -0.0083,  0.0231,  0.0320],\n",
            "         [-0.1628, -0.0792,  0.0208,  ...,  0.0155, -0.0019,  0.0378]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Feed-Forward Layer and Normalization"
      ],
      "metadata": {
        "id": "-gFOxK9yhy5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The FeedForward class represents the feed-forward neural network component commonly found in transformer-based models. This component consists of two linear layers with a GELU activation function and dropout applied in between.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_size (int): The dimensionality of the input and output tensors.\n",
        "        intermediate_fc_size (int): The size of the intermediate fully connected layer, typically set to four times the hidden size.\n",
        "        hidden_dropout_prob (float): The dropout probability applied after the GELU activation function.\n",
        "\n",
        "    Methods:\n",
        "        forward(hidden_state: torch.Tensor) -> torch.Tensor\n",
        "            Performs the forward pass through the feed-forward network, applying linear transformations, activation function, and dropout.\n",
        "\n",
        "    Args:\n",
        "        config (object): A configuration object containing the attributes necessary to initialize the feed-forward network. These attributes include:\n",
        "            - hidden_size: The dimensionality of the input and output tensors.\n",
        "            - hidden_dropout_prob: The dropout probability applied after the GELU activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_fc_size = self.hidden_size * 4\n",
        "        self.hidden_dropout_prob = config.hidden_dropout_prob\n",
        "\n",
        "        # Define the linear layers and dropout\n",
        "        self.fc1 = nn.Linear(self.hidden_size, self.intermediate_fc_size)\n",
        "        self.fc2 = nn.Linear(self.intermediate_fc_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the feed-forward network.\n",
        "\n",
        "        Args:\n",
        "            hidden_state (torch.Tensor): The input tensor representing hidden states from the previous layer.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after passing through the feed-forward network.\n",
        "        \"\"\"\n",
        "        hidden_state = self.fc1(hidden_state)\n",
        "        hidden_state = F.gelu(hidden_state)\n",
        "        hidden_state = self.dropout(hidden_state)\n",
        "        hidden_state = self.fc2(hidden_state)\n",
        "        return hidden_state\n",
        "\n"
      ],
      "metadata": {
        "id": "oOnWkhDuhzuQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "QLfiT-w6h3xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config: object) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_size: int = config.hidden_size\n",
        "        self.hidden_dropout_prob: float = config.hidden_dropout_prob\n",
        "\n",
        "        self.multihead_attention: MultiHeadAttention = MultiHeadAttention(config)\n",
        "        self.feed_forward: FeedForward = FeedForward(config)\n",
        "\n",
        "        self.norm1: nn.LayerNorm = nn.LayerNorm(self.hidden_size, eps=1e-6)\n",
        "        self.norm2: nn.LayerNorm = nn.LayerNorm(self.hidden_size, eps=1e-6)\n",
        "        self.dropout1: nn.Dropout = nn.Dropout(self.hidden_dropout_prob)\n",
        "        self.dropout2: nn.Dropout = nn.Dropout(self.hidden_dropout_prob)\n",
        "        self.dropout_ffn: nn.Dropout = nn.Dropout(self.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self,\n",
        "                hidden_state: torch.Tensor,\n",
        "                biases: torch.Tensor,\n",
        "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Processes input hidden states through an encoder block, applying self-attention,\n",
        "        feed-forward network, normalization, and dropout.\n",
        "\n",
        "        Args:\n",
        "            hidden_state (torch.Tensor): The input tensor containing hidden states for each token in the input sequence.\n",
        "            biases (torch.Tensor): The bias tensor used in the self-attention mechanism to prevent attention to certain positions.\n",
        "            mask (Optional[torch.Tensor]): An optional mask tensor to apply during the self-attention mechanism,\n",
        "                                            allowing the model to ignore specific tokens for attention calculations\n",
        "                                            (e.g., padding tokens).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after processing through the encoder block,\n",
        "                          with the same shape as the input hidden_state tensor.\n",
        "        \"\"\"\n",
        "        normed_hidden_state: torch.Tensor = self.norm1(hidden_state)\n",
        "        attention_output: torch.Tensor = self.multihead_attention(\n",
        "            query=normed_hidden_state,\n",
        "            key=normed_hidden_state,\n",
        "            value=normed_hidden_state,\n",
        "            relative_position_bias=biases,\n",
        "            mask=mask\n",
        "        )\n",
        "        attention_output: torch.Tensor = self.dropout1(attention_output)\n",
        "        hidden_state: torch.Tensor = hidden_state + attention_output\n",
        "\n",
        "        normed_hidden_state: torch.Tensor = self.norm2(hidden_state)\n",
        "        feed_forward_output: torch.Tensor = self.feed_forward(normed_hidden_state)\n",
        "        feed_forward_output: torch.Tensor = self.dropout2(feed_forward_output)\n",
        "        hidden_state: torch.Tensor = hidden_state + feed_forward_output\n",
        "\n",
        "        hidden_state: torch.Tensor = self.dropout_ffn(hidden_state)\n",
        "\n",
        "        return hidden_state\n"
      ],
      "metadata": {
        "id": "MfkmIVFbVhCd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask(size, length):\n",
        "    \"\"\"Function to generate a mask with a specific size and length.\"\"\"\n",
        "    mask = torch.ones(size, dtype=torch.long)\n",
        "    mask[:, :length] = 0\n",
        "    return mask\n",
        "\n",
        "def test_encoder():\n",
        "    config = MockConfig()\n",
        "\n",
        "    encoder = Encoder(config)\n",
        "\n",
        "    batch_size = 2\n",
        "    seq_length = config.max_token_len\n",
        "    hidden_state = torch.rand(batch_size, seq_length, config.hidden_size)\n",
        "    mask = generate_mask((batch_size, seq_length), 5)\n",
        "\n",
        "    relative_position_bias_module = RelativePositionBias(config)\n",
        "    biases = relative_position_bias_module(seq_length, seq_length)\n",
        "\n",
        "    output = encoder(hidden_state, biases, mask)\n",
        "\n",
        "    print(\"Encoder output shape:\", output.shape)\n",
        "    print(\"Encoder output:\", output)\n",
        "if __name__ == \"__main__\":\n",
        "    test_encoder()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS1NRv1o3G48",
        "outputId": "2bcf2ccf-191d-4b01-a4a2-eb8a0d0bc267"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([2, 50, 128])\n",
            "Encoder output: tensor([[[ 6.5703e-01,  7.5853e-01,  7.6513e-01,  ..., -6.8949e-04,\n",
            "           7.3960e-01,  6.6737e-01],\n",
            "         [ 5.7515e-01,  5.4225e-01,  7.9499e-01,  ...,  0.0000e+00,\n",
            "           2.4748e-01,  1.1371e+00],\n",
            "         [ 1.5329e-02,  1.1174e-01,  9.1193e-01,  ...,  7.1894e-01,\n",
            "           4.7090e-01,  1.1896e+00],\n",
            "         ...,\n",
            "         [-0.0000e+00,  7.3668e-01,  4.7978e-01,  ...,  3.9750e-01,\n",
            "           8.7259e-01,  1.1742e+00],\n",
            "         [ 4.9656e-02,  1.3127e+00,  2.9293e-01,  ...,  2.6353e-01,\n",
            "          -2.5861e-01,  4.8397e-01],\n",
            "         [ 6.8506e-01,  7.8986e-01,  7.7735e-01,  ...,  9.5067e-01,\n",
            "          -1.6525e-01,  0.0000e+00]],\n",
            "\n",
            "        [[-3.6254e-02,  2.2118e-02,  2.5812e-01,  ...,  5.3448e-01,\n",
            "           0.0000e+00,  9.2391e-01],\n",
            "         [-7.8836e-02,  1.2323e+00,  5.6941e-01,  ...,  3.5727e-01,\n",
            "           8.0358e-01,  1.3043e+00],\n",
            "         [ 3.6691e-01,  7.1192e-02,  7.0225e-01,  ..., -0.0000e+00,\n",
            "           1.2780e-01,  1.0731e+00],\n",
            "         ...,\n",
            "         [ 9.1965e-01,  5.0450e-01,  1.5096e+00,  ...,  8.2499e-01,\n",
            "          -2.5107e-01,  6.2380e-01],\n",
            "         [ 3.6819e-01,  4.6103e-01,  0.0000e+00,  ...,  8.8879e-02,\n",
            "           4.2247e-01,  9.9482e-01],\n",
            "         [ 2.4899e-02,  4.4069e-01,  1.5994e-01,  ...,  0.0000e+00,\n",
            "          -3.3072e-01,  1.0549e+00]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "8qq7uRLGmmZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3nWDYtxpg4i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from attention import MultiHeadAttention\n",
        "#from feed_forward import FeedForward\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder class represents a single decoder block within a transformer-based model. This block is designed to process input sequences with attention to both the output of the encoder and its own previous outputs. The decoder employs masked multi-head self-attention to prevent positions from attending to subsequent positions. This is crucial for preserving the auto-regressive property in tasks like language modeling.\n",
        "\n",
        "    Attributes:\n",
        "        masked_multihead_attention (MultiHeadAttention): The masked multi-head self-attention mechanism that allows the decoder to focus on different parts of the decoder's input sequence without looking ahead to future tokens.\n",
        "        multihead_attention (MultiHeadAttention): The multi-head attention mechanism that focuses on the encoder's output, facilitating the integration of context from the encoder.\n",
        "        feed_forward (FeedForward): The position-wise feed-forward neural network applied after the attention mechanisms.\n",
        "        norm1, norm2, norm3 (nn.LayerNorm): Layer normalization applied before and after the self-attention and encoder-decoder attention mechanisms, as well as before the feed-forward network.\n",
        "        dropout1, dropout2, dropout3, dropout_ffn (nn.Dropout): Dropout layers applied after each attention mechanism and the feed-forward network to prevent overfitting.\n",
        "\n",
        "    Methods:\n",
        "        forward(hidden_state, encoder_info, biases, mask=None) -> torch.Tensor\n",
        "            Performs a forward pass through the decoder block, processing the input hidden states with masked self-attention, encoder-decoder attention, and feed-forward layers.\n",
        "\n",
        "        get_causal_attention_mask(input_shape) -> torch.Tensor\n",
        "            Generates a causal attention mask to prevent decoder tokens from attending to future tokens in the sequence.\n",
        "\n",
        "    Args:\n",
        "        config (object): A configuration object containing hyperparameters for initializing the decoder block components. These include the hidden layer size (`hidden_size`), dropout probability (`hidden_dropout_prob`), and other parameters required by the `MultiHeadAttention` and `FeedForward` modules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Initialize the attention mechanisms and feed-forward network\n",
        "        self.masked_multihead_attention = MultiHeadAttention(config)\n",
        "        self.multihead_attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "        # Layer normalization and dropout for stabilization and regularization\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.norm3 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dropout2 = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dropout3 = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dropout_ffn = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_state, encoder_info, biases, mask=None):\n",
        "        \"\"\"\n",
        "        Processes input hidden states through a decoder block.\n",
        "\n",
        "        Args:\n",
        "            hidden_state (torch.Tensor): The input tensor containing hidden states for each token in the decoder's input sequence.\n",
        "            encoder_info (torch.Tensor): The output tensor from the encoder, containing context for each token in the encoder's input sequence.\n",
        "            biases (torch.Tensor): The bias tensor used in the self-attention mechanism to prevent attention to certain positions.\n",
        "            mask (torch.Tensor, optional): An optional mask tensor to apply during the self-attention mechanism, allowing the model to ignore specific tokens for attention calculations (e.g., padding tokens).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after processing through the decoder block, with the same shape as the input hidden_state tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        input_shape = hidden_state.size()\n",
        "        causal_mask = self.get_causal_attention_mask(input_shape)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = mask.unsqueeze(1).expand_as(causal_mask)\n",
        "            causal_mask = torch.min(padding_mask, causal_mask)\n",
        "\n",
        "        normed_hidden_state = self.norm1(hidden_state)\n",
        "        attention_output = self.masked_multihead_attention(normed_hidden_state, normed_hidden_state, normed_hidden_state, biases, causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        hidden_state = attention_output + hidden_state\n",
        "        #print(attention_output.shape)\n",
        "        normed_hidden_state = self.norm2(hidden_state)\n",
        "        attention_output = self.multihead_attention(normed_hidden_state, encoder_info, encoder_info, biases, padding_mask)\n",
        "\n",
        "        attention_output = self.dropout2(attention_output)\n",
        "        hidden_state = attention_output + hidden_state\n",
        "\n",
        "        normed_hidden_state = self.norm3(hidden_state)\n",
        "        feed_forward_output = self.feed_forward(normed_hidden_state)\n",
        "        feed_forward_output = self.dropout3(feed_forward_output)\n",
        "        hidden_state = feed_forward_output + hidden_state\n",
        "\n",
        "        hidden_state = self.dropout_ffn(hidden_state)\n",
        "\n",
        "        return hidden_state\n",
        "\n",
        "    def get_causal_attention_mask(self, input_shape):\n",
        "          \"\"\"\n",
        "          Generates the causal attention mask for PyTorch.\n",
        "\n",
        "          Args:\n",
        "              inputs (torch.Tensor): Input tensor of shape [batch_size, sequence_length, ...].\n",
        "\n",
        "          Returns:\n",
        "              torch.Tensor: Causal attention mask tensor of shape [batch_size, sequence_length, sequence_length].\n",
        "          \"\"\"\n",
        "          batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "          device = next(self.parameters()).device\n",
        "          i = torch.arange(sequence_length, device=device).unsqueeze(1).expand(sequence_length, sequence_length)\n",
        "          j = torch.arange(sequence_length, device=device).unsqueeze(0).expand(sequence_length, sequence_length)\n",
        "          mask = (i >= j).int()\n",
        "          mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "          return mask"
      ],
      "metadata": {
        "id": "qgh1bK0M8n0I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decoder():\n",
        "    config = MockConfig()\n",
        "\n",
        "    decoder = Decoder(config)\n",
        "\n",
        "    batch_size = 1\n",
        "    seq_length = 5\n",
        "    hidden_state = torch.rand(batch_size, seq_length, config.hidden_size)\n",
        "    mask = generate_mask((batch_size, seq_length), 3)\n",
        "\n",
        "    relative_position_bias_module = RelativePositionBias(config)\n",
        "    biases = relative_position_bias_module(seq_length, seq_length)\n",
        "\n",
        "    encoder_info = torch.rand(batch_size, seq_length, config.hidden_size)\n",
        "\n",
        "    output = decoder(hidden_state, encoder_info, biases, mask)\n",
        "\n",
        "    print(\"Decoder output shape:\", output.shape)\n",
        "    print(\"Decoder output shape:\", output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVFXr2WG4-ia",
        "outputId": "2246c786-b843-4cae-da07-26e701381ec6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: torch.Size([1, 5, 128])\n",
            "Decoder output shape: tensor([[[    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan],\n",
            "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan],\n",
            "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "              nan,     nan],\n",
            "         [ 0.0000,  0.8828,  1.0093,  0.5932,  0.5055,  0.4696,  0.3449,\n",
            "           0.6810, -0.1280,  0.7526,  0.6772,  0.8492, -0.8962, -0.0000,\n",
            "           2.0708,  1.1412,  0.0000, -0.6533,  0.4214,  0.7866,  0.5472,\n",
            "           1.0894,  0.4445,  1.2364,  1.1906, -0.0000,  0.6026,  0.1767,\n",
            "          -0.3254,  0.4069,  0.6195,  0.3574,  1.9849,  1.5092,  0.7060,\n",
            "           1.4449,  1.0297,  0.6556,  0.0955,  1.0418, -0.9122, -0.5131,\n",
            "           1.5627,  0.7130,  0.1406,  0.2565,  0.3299, -0.5762,  1.0377,\n",
            "          -0.0000,  1.1075,  0.0000, -0.1189,  0.5666,  0.0000,  1.3485,\n",
            "           0.1829, -0.1914,  0.3826,  0.4982,  0.6857,  1.4552,  0.0000,\n",
            "           0.0738, -0.4991,  0.0000,  1.7523,  0.1874,  0.1426,  1.2152,\n",
            "           0.7023,  0.6869,  1.0517,  1.0505,  0.5688, -0.0000, -0.0000,\n",
            "           0.0000,  0.9415,  0.7324,  1.4625, -2.4086,  0.0388,  0.2081,\n",
            "           0.0698,  0.0669,  2.3348,  1.5121,  0.7947,  0.7656,  0.4688,\n",
            "           0.8619,  0.0000,  0.9760,  1.0971, -0.4294,  0.5144, -0.0923,\n",
            "           0.5467,  0.0000,  0.2786, -0.0727,  0.0000,  1.1177,  2.1704,\n",
            "           0.6401,  0.6105, -0.8076,  0.3841,  0.3916,  0.1547, -0.2361,\n",
            "           0.0000,  1.4454,  0.4874,  0.9501, -0.1547,  1.9426,  0.3177,\n",
            "           0.7079,  0.1686,  1.1957,  0.9086,  0.0179,  0.6227,  0.0099,\n",
            "          -0.0000, -0.9443],\n",
            "         [ 0.6229,  0.6127,  1.2801,  0.4262,  0.8851,  0.0000, -0.6106,\n",
            "           0.0633,  0.5342,  1.5511,  0.2118,  0.9897,  0.0372,  1.2914,\n",
            "           1.0865,  0.9597,  0.3007, -1.4839,  0.1991,  0.4337,  1.1398,\n",
            "           0.7360,  0.8077,  0.1514,  0.3260, -0.0000,  0.0000,  0.6768,\n",
            "          -0.1798, -0.4649,  0.6404,  0.3660,  0.2306,  1.0039, -0.1239,\n",
            "           0.9667,  1.0968,  1.0646, -0.1050, -0.2669,  0.5261,  0.0670,\n",
            "           0.0000,  0.1041, -0.3085,  0.0000,  1.0954, -0.4735,  1.6031,\n",
            "          -0.3114,  1.6794,  0.3213,  0.6431,  0.5481,  0.4414,  0.3096,\n",
            "           0.3620, -0.5907,  0.6639,  0.0000, -0.0000,  0.8492, -0.0614,\n",
            "          -0.9003,  0.6914,  0.7786,  0.4009,  0.6256,  1.4598,  1.2764,\n",
            "           0.1627,  0.5581, -0.0811,  0.7888, -0.7781,  0.9157,  0.0000,\n",
            "           0.4590,  0.1405,  0.0000,  0.0000, -0.5461,  0.9155,  0.6284,\n",
            "          -0.3923, -0.6408,  1.3561,  0.6627,  1.5275,  0.6089,  0.9256,\n",
            "           0.7094,  0.8227, -0.1037,  0.4277,  0.1408,  0.9988,  0.5865,\n",
            "           1.5205,  1.0321,  0.4888,  0.2627,  0.4950,  0.5796,  0.0000,\n",
            "          -0.6291,  0.4668, -0.0000,  0.6338,  0.1296,  0.0000, -0.1032,\n",
            "          -0.0785,  0.4521,  1.1337,  1.0917,  0.3176,  1.7354,  0.0000,\n",
            "           0.0000,  0.2641,  0.0000,  0.3511,  0.6237,  0.5647, -0.0825,\n",
            "           0.6129, -0.3796]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "DGJMmi_UmK2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from encoder import Encoder\n",
        "#from decoder import Decoder\n",
        "#from embeddings import Embeddings\n",
        "#from relative_position_bias import RelativePositionBias\n",
        "\n",
        "class T5Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a simplified version of the T5 model, a transformer-based model designed for a variety of NLP tasks. This model consists of an embedding layer, multiple encoder and decoder blocks, and a final prediction layer. The model is designed to process sequences of tokens, encoding them into a latent space representation which is then decoded into an output sequence.\n",
        "\n",
        "    The architecture is modular, allowing for a configurable number of encoder and decoder blocks. Each block in the encoder and decoder is capable of self-attention and feed-forward neural network processing. The model also incorporates relative position biases to account for the positions of tokens within the sequence.\n",
        "\n",
        "    Attributes:\n",
        "        num_blocks (int): The number of encoder and decoder blocks to include in the model.\n",
        "        vocab_size (int): The size of the vocabulary used in the embeddings layer.\n",
        "        hidden_size (int): The dimensionality of the hidden layers and embeddings.\n",
        "        embed_layer (Embeddings): The initial embedding layer for input tokens.\n",
        "        relative_position_bias (RelativePositionBias): Module for calculating relative position biases used in attention mechanisms.\n",
        "        biases (torch.Tensor): Pre-computed biases for all positions up to a maximum token length.\n",
        "        encoder (nn.ModuleList): A list of encoder blocks.\n",
        "        decoder (nn.ModuleList): A list of decoder blocks.\n",
        "        prediction_layer (nn.Linear): A linear layer that projects decoder output to the vocabulary size.\n",
        "        softmax (nn.LogSoftmax): The softmax layer applied to the outputs of the prediction layer.\n",
        "\n",
        "    Methods:\n",
        "        forward(input_ids, labels, mask) -> Tuple[torch.Tensor, torch.Tensor]\n",
        "            Processes input token IDs and labels through the model, returning logits for the predicted token IDs.\n",
        "        to(*args, **kwargs)\n",
        "            Overrides the `.to()` method to ensure all parts of the model, including manually managed tensors, are moved to the specified device.\n",
        "\n",
        "    Args:\n",
        "        config (object): A configuration object with hyperparameters for model components. Expected attributes include `num_blocks`, `vocab_size`, `hidden_size`, among others necessary for initializing submodules.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(T5Model, self).__init__()\n",
        "\n",
        "        self.num_blocks: int = 1#config.num_blocks\n",
        "        self.vocab_size: int = config.vocab_size\n",
        "        self.hidden_size: int = config.hidden_size\n",
        "        self.max_token_len: int = config.max_token_len\n",
        "\n",
        "        self.embed_layer: Embeddings = Embeddings(config)\n",
        "        self.relative_position_bias = RelativePositionBias(config)\n",
        "\n",
        "        self.encoder: nn.ModuleList = nn.ModuleList([Encoder(config) for _ in range(self.num_blocks)])\n",
        "        self.decoder: nn.ModuleList = nn.ModuleList([Decoder(config) for _ in range(self.num_blocks)])\n",
        "\n",
        "        # The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix.\n",
        "        self.prediction_layer: nn.Linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self.prediction_layer.weight = self.embed_layer.token_embeddings.weight\n",
        "        self.softmax: nn.LogSoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Processes input and label sequences through the T5 model architecture, returning the logits of the predicted output sequence.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Tensor of input token IDs.\n",
        "            labels (torch.Tensor): Tensor of target token IDs for teacher forcing during training.\n",
        "            mask (torch.Tensor): Attention mask tensor to specify which positions should be attended to and which should not.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The logits representing the model's predictions for the next token in the sequence.\n",
        "        \"\"\"\n",
        "        x_enc: torch.Tensor  = self.embed_layer(input_ids)\n",
        "        x_dec: torch.Tensor  = self.embed_layer(labels)\n",
        "        biases = self.relative_position_bias(self.max_token_len, self.max_token_len)\n",
        "\n",
        "        for encoder_layer in self.encoder:\n",
        "            x_enc: torch.Tensor = encoder_layer(x_enc, biases, mask)\n",
        "\n",
        "        for decoder_layer in self.decoder:\n",
        "            x_dec = decoder_layer(x_dec, x_enc, biases, mask)\n",
        "\n",
        "        x_logits: torch.Tensor = self.prediction_layer(x_dec)\n",
        "\n",
        "        return self.softmax(x_logits)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "45ILpA8Qmjha"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_t5_model():\n",
        "    config = MockConfig()\n",
        "    model = T5Model(config)\n",
        "\n",
        "    batch_size = 1\n",
        "    seq_length = 50\n",
        "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
        "    labels = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
        "    mask = generate_mask((batch_size, seq_length), 30)\n",
        "\n",
        "    x = model(input_ids, labels, mask)\n",
        "\n",
        "    # x shape should be (batch_size, seq_length, vocab_size)\n",
        "    assert x.size() == (batch_size, seq_length, config.vocab_size), f\"Unexpected output shape: {x.size()}\"\n",
        "\n",
        "    print(\"Test passed! Output shape:\", x.size())\n",
        "    print(\"Output:\", x)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_t5_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-23SgBvX_LCF",
        "outputId": "84323816-937b-4479-c594-bcd104a37e4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed! Output shape: torch.Size([1, 50, 10000])\n",
            "Output: tensor([[[      nan,       nan,       nan,  ...,       nan,       nan,\n",
            "                nan],\n",
            "         [      nan,       nan,       nan,  ...,       nan,       nan,\n",
            "                nan],\n",
            "         [      nan,       nan,       nan,  ...,       nan,       nan,\n",
            "                nan],\n",
            "         ...,\n",
            "         [-101.8985, -128.6233, -132.1542,  ..., -125.4327, -109.3851,\n",
            "          -101.8341],\n",
            "         [-111.2209, -142.6120, -117.0396,  ..., -123.0658, -107.2759,\n",
            "          -100.3267],\n",
            "         [-132.1396, -145.1212, -141.1664,  ..., -134.1875, -141.5103,\n",
            "          -138.6976]]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **End-to-End T5**"
      ],
      "metadata": {
        "id": "L7cnUTlzoOVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optim"
      ],
      "metadata": {
        "id": "BeH_0GTMoKAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class ScheduledOptim:\n",
        "    \"\"\"\n",
        "    A wrapper class around an optimizer to manage learning rate scheduling.\n",
        "\n",
        "    This implements an \"inverse square root\" schedule with a warm-up phase,\n",
        "    which sets a constant learning rate for the first N steps, followed by a\n",
        "    decay phase where the learning rate decreases according to the inverse square root\n",
        "    of the step number.\n",
        "\n",
        "    Attributes:\n",
        "        optimizer (Optimizer): The wrapped optimizer.\n",
        "        d_model (int): The dimensionality of the model's hidden layers.\n",
        "        n_warmup_steps (int): The number of steps to apply the warm-up.\n",
        "        n_current_steps (int): The current step in the optimization process.\n",
        "\n",
        "    Methods:\n",
        "        step_and_update_lr(): Performs a single optimization step and updates\n",
        "            the learning rate according to the schedule.\n",
        "        zero_grad(): Clears the gradients of all optimized parameters.\n",
        "        calculate_lr(): Computes the current learning rate according to the\n",
        "            schedule.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer: 'Optimizer', d_model: int, n_warmup_steps: int = 10000) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the ScheduledOptim instance.\n",
        "\n",
        "        Args:\n",
        "            optimizer (Optimizer): The optimizer to wrap.\n",
        "            d_model (int): The dimensionality of the model's hidden layers.\n",
        "            n_warmup_steps (int): The number of steps over which to linearly increase the learning rate.\n",
        "        \"\"\"\n",
        "        self.optimizer: 'Optimizer' = optimizer\n",
        "        self.d_model: int = d_model\n",
        "        self.n_warmup_steps: int = n_warmup_steps\n",
        "        self.n_current_steps: int = 0\n",
        "\n",
        "    def step_and_update_lr(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs an optimization step and updates the learning rate according\n",
        "        to the current step using an inverse square root schedule.\n",
        "        \"\"\"\n",
        "        self.n_current_steps += 1\n",
        "        lr: float = self.calculate_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print(\"LR:\",lr)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"\n",
        "        Clears the gradients of all optimized parameters by calling zero_grad\n",
        "        on the wrapped optimizer.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def calculate_lr(self) -> float:\n",
        "      \"\"\"\n",
        "      Calculates the learning rate using an \"inverse square root\" schedule\n",
        "      with a warm-up phase, where the learning rate is constant during the\n",
        "      warm-up and then decreases according to the inverse square root of\n",
        "      the step number.\n",
        "\n",
        "      Returns:\n",
        "          float: The calculated learning rate.\n",
        "      \"\"\"\n",
        "      step = self.n_current_steps\n",
        "      warmup = self.n_warmup_steps\n",
        "\n",
        "      if step <= warmup:\n",
        "          # Constant learning rate during warm-up phase\n",
        "          lr = 0.01\n",
        "      else:\n",
        "          # Decay phase starts after the warm-up\n",
        "          factor = self.d_model ** (-0.5)\n",
        "          lr = factor * ((step - warmup) ** -0.5) * (warmup ** 0.5)\n",
        "\n",
        "      return lr"
      ],
      "metadata": {
        "id": "IX2mxvaqMq_t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaFactor"
      ],
      "metadata": {
        "id": "iISfzb7AjapS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import math\n",
        "#import torch\n",
        "#from typing import Any, Dict, Tuple\n",
        "#from torch.optim.optimizer import Optimizer\n",
        "#from types import OptFloat, OptLossClosure, Params, State\n",
        "\n",
        "\n",
        "Params = Union[Iterable[Tensor], Iterable[Dict[str, Any]]]\n",
        "LossClosure = Callable[[], float]\n",
        "OptLossClosure = Optional[LossClosure]\n",
        "Betas2 = Tuple[float, float]\n",
        "State = Dict[str, Any]\n",
        "OptFloat = Optional[float]\n",
        "Nus2 = Tuple[float, float]\n",
        "\n",
        "Eps2 = Tuple[float, float]\n",
        "ParamGroup = Dict[str, Any]\n",
        "\n",
        "class Adafactor(Optimizer):\n",
        "    \"\"\"Implements Adafactor algorithm.\n",
        "\n",
        "    It has been proposed in: `Adafactor: Adaptive Learning Rates with\n",
        "    Sublinear Memory Cost`__.\n",
        "\n",
        "    Arguments:\n",
        "        params: iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr: external learning rate (default: None)\n",
        "        eps2: regularization constans for square gradient\n",
        "            and parameter scale respectively (default: (1e-30, 1e-3))\n",
        "        clip_threshold: threshold of root mean square of\n",
        "            final gradient update (default: 1.0)\n",
        "        decay_rate: coefficient used to compute running averages of square\n",
        "            gradient (default: -0.8)\n",
        "        beta1: coefficient used for computing running averages of gradient\n",
        "            (default: None)\n",
        "        weight_decay: weight decay (L2 penalty) (default: 0)\n",
        "        scale_parameter: if true, learning rate is scaled by root mean square\n",
        "            of parameter (default: True)\n",
        "        relative_step: if true, time-dependent learning rate is computed\n",
        "            instead of external learning rate (default: True)\n",
        "        warmup_init: time-dependent learning rate computation depends on\n",
        "            whether warm-up initialization is being used (default: False)\n",
        "\n",
        "    Example:\n",
        "        >>> import torch_optimizer as optim\n",
        "        >>> optimizer = optim.Adafactor(model.parameters())\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "\n",
        "    __ https://arxiv.org/abs/1804.04235\n",
        "\n",
        "    Note:\n",
        "        Reference code: https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py  # noqa\n",
        "        https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/adafactor.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Params,\n",
        "        lr: OptFloat = None,\n",
        "        eps2: Eps2 = (1e-30, 1e-3),\n",
        "        clip_threshold: float = 1.0,\n",
        "        decay_rate: float = -0.8,\n",
        "        beta1: OptFloat = None,\n",
        "        weight_decay: float = 0.0,\n",
        "        scale_parameter: bool = True,\n",
        "        relative_step: bool = True,\n",
        "        warmup_init: bool = False,\n",
        "    ):\n",
        "        if lr is not None and lr <= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\n",
        "                \"Invalid weight_decay value: {}\".format(weight_decay)\n",
        "            )\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            eps2=eps2,\n",
        "            clip_threshold=clip_threshold,\n",
        "            decay_rate=decay_rate,\n",
        "            beta1=beta1,\n",
        "            weight_decay=weight_decay,\n",
        "            scale_parameter=scale_parameter,\n",
        "            relative_step=relative_step,\n",
        "            warmup_init=warmup_init,\n",
        "        )\n",
        "        super(Adafactor, self).__init__(params, defaults)\n",
        "\n",
        "    def _get_lr(self, param_group: ParamGroup, param_state: State) -> float:\n",
        "        rel_step_sz = param_group[\"lr\"]\n",
        "        if param_group[\"relative_step\"]:\n",
        "            min_step = (\n",
        "                1e-6 * param_state[\"step\"]\n",
        "                if param_group[\"warmup_init\"]\n",
        "                else 1e-2\n",
        "            )\n",
        "            rel_step_sz = min(min_step, 1.0 / param_state[\"step\"]**-0.5)\n",
        "        param_scale = 1.0\n",
        "        if param_group[\"scale_parameter\"]:\n",
        "            param_scale = max(param_group[\"eps2\"][1], param_state[\"RMS\"])\n",
        "        return param_scale * rel_step_sz\n",
        "\n",
        "    def _get_options(\n",
        "        self, param_group: ParamGroup, param_shape: Tuple[int, ...]\n",
        "    ) -> Tuple[bool, bool]:\n",
        "        factored = len(param_shape) >= 2\n",
        "        use_first_moment = param_group[\"beta1\"] is not None\n",
        "        return factored, use_first_moment\n",
        "\n",
        "    def _rms(self, tensor: torch.Tensor) -> float:\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def _approx_sq_grad(\n",
        "        self,\n",
        "        exp_avg_sq_row: torch.Tensor,\n",
        "        exp_avg_sq_col: torch.Tensor,\n",
        "        output: torch.Tensor,\n",
        "    ) -> None:\n",
        "        r_factor = (\n",
        "            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1))\n",
        "            .rsqrt_()\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()\n",
        "        torch.mul(r_factor, c_factor, out=output)\n",
        "\n",
        "    def step(self, closure: OptLossClosure = None) -> OptFloat:\n",
        "        r\"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure: A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        \"Adafactor does not support sparse gradients.\"\n",
        "                    )\n",
        "\n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                factored, use_first_moment = self._get_options(\n",
        "                    group, grad_shape\n",
        "                )\n",
        "                # State Initialization\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                    if use_first_moment:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state[\"exp_avg\"] = torch.zeros_like(\n",
        "                            grad, memory_format=torch.preserve_format\n",
        "                        )\n",
        "                    if factored:\n",
        "                        state[\"exp_avg_sq_row\"] = torch.zeros(\n",
        "                            grad_shape[:-1]\n",
        "                        ).type_as(grad)\n",
        "                        state[\"exp_avg_sq_col\"] = torch.zeros(\n",
        "                            grad_shape[:-2] + grad_shape[-1:]\n",
        "                        ).type_as(grad)\n",
        "                    else:\n",
        "                        state[\"exp_avg_sq\"] = torch.zeros_like(\n",
        "                            grad, memory_format=torch.preserve_format\n",
        "                        )\n",
        "\n",
        "                    state[\"RMS\"] = 0\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                state[\"RMS\"] = self._rms(p.data)\n",
        "                lr = self._get_lr(group, state)\n",
        "\n",
        "                beta2t = 1.0 - math.pow(state[\"step\"], group[\"decay_rate\"])\n",
        "                update = (grad**2) + group[\"eps2\"][0]\n",
        "                if factored:\n",
        "                    exp_avg_sq_row = state[\"exp_avg_sq_row\"]\n",
        "                    exp_avg_sq_col = state[\"exp_avg_sq_col\"]\n",
        "\n",
        "                    exp_avg_sq_row.mul_(beta2t).add_(\n",
        "                        update.mean(dim=-1), alpha=1.0 - beta2t\n",
        "                    )\n",
        "                    exp_avg_sq_col.mul_(beta2t).add_(\n",
        "                        update.mean(dim=-2), alpha=1.0 - beta2t\n",
        "                    )\n",
        "\n",
        "                    # Approximation of exponential moving average of square\n",
        "                    # of gradient\n",
        "                    self._approx_sq_grad(\n",
        "                        exp_avg_sq_row, exp_avg_sq_col, update\n",
        "                    )\n",
        "                    update.mul_(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "                    exp_avg_sq.mul_(beta2t).add_(update, alpha=1.0 - beta2t)\n",
        "                    torch.rsqrt(exp_avg_sq, out=update).mul_(grad)\n",
        "\n",
        "                update.div_(\n",
        "                    max(1.0, self._rms(update) / group[\"clip_threshold\"])\n",
        "                )\n",
        "                update.mul_(lr)\n",
        "\n",
        "                if use_first_moment:\n",
        "                    exp_avg = state[\"exp_avg\"]\n",
        "                    exp_avg.mul_(group[\"beta1\"]).add_(\n",
        "                        update, alpha=1 - group[\"beta1\"]\n",
        "                    )\n",
        "                    update = exp_avg\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p.data.add_(p.data, alpha=-group[\"weight_decay\"] * lr)\n",
        "\n",
        "                p.data.add_(-update)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "b2NRruzRhRgv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training class"
      ],
      "metadata": {
        "id": "ULRZ8O3_otbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from typing import Tuple, List, Union\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from torch.utils.data import DataLoader\n",
        "#from scheduled_optim import ScheduledOptim\n",
        "#from config import Config\n",
        "#from t5_model import T5Model\n",
        "\n",
        "class T5Trainer:\n",
        "    \"\"\"\n",
        "    A trainer class for T5Model, designed to encapsulate the training and testing routines. It manages the training epochs,\n",
        "    data loading, model optimization, and evaluation, providing a streamlined workflow for experimenting with the T5Model.\n",
        "    The trainer supports both training and testing phases, handling the forward pass, loss computation, gradient backpropagation,\n",
        "    and parameter updates.\n",
        "\n",
        "    Attributes:\n",
        "        model (T5Model): The T5Model instance to be trained and evaluated.\n",
        "        log_freq (int): Frequency of logging training progress (number of iterations).\n",
        "        batch_size (int): The size of input data batches for training and testing.\n",
        "        save_path (str): Path where the trained model checkpoints will be saved.\n",
        "        device (torch.device): The device (CPU/GPU) on which the model and data should be loaded.\n",
        "        train_data (DataLoader): DataLoader instance providing access to the training data.\n",
        "        test_data (DataLoader, optional): DataLoader instance providing access to the test data. Default is None.\n",
        "        optim (ScheduledOptim): The optimizer with a learning rate scheduling mechanism used for training.\n",
        "        criterion (nn.NLLLoss): The loss function used for model training.\n",
        "\n",
        "    Args:\n",
        "        config (Config): Configuration object containing model, training, and optimization settings.\n",
        "        t5 (T5Model): The T5Model instance to be trained and evaluated.\n",
        "        optim (ScheduledOptim): The optimizer with a learning rate scheduling mechanism used for training.\n",
        "        device (torch.device): The device (CPU/GPU) on which the model and data should be loaded.\n",
        "        train_dataloader (DataLoader): DataLoader instance providing access to the training data.\n",
        "        test_dataloader (DataLoader, optional): DataLoader instance providing access to the test data. Default is None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config, t5: T5Model, optim: ScheduledOptim, device: torch.device,\n",
        "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None):\n",
        "        self.model = t5\n",
        "        self.log_freq: int = config.log_freq\n",
        "        self.batch_size: int = config.batch_size\n",
        "        self.save_path: str = config.save_path\n",
        "        self.device = device\n",
        "        # Setting the train and test data loader\n",
        "        self.train_data: DataLoader = train_dataloader\n",
        "        self.test_data: DataLoader = test_dataloader\n",
        "        self.optim: ScheduledOptim = optim\n",
        "\n",
        "        # Using Negative Log Likelihood Loss function\n",
        "        self.criterion: nn.NLLLoss = nn.NLLLoss(ignore_index=-100)\n",
        "\n",
        "\n",
        "        print(\"Total Parameters:\", sum(p.nelement() for p in self.model.parameters()))\n",
        "\n",
        "    def train(self, epoch: int) -> None:\n",
        "        \"\"\"\n",
        "        Train the T5Model for one epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current epoch number.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.iteration(epoch, self.train_data, train=True)\n",
        "\n",
        "    def test(self, epoch: int) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates the T5Model on the test dataset, if provided.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current epoch number.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            self.iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "    def iteration(self, epoch: int, data_iter: DataLoader, train: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Perform an iteration of training or testing.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Current epoch number.\n",
        "            data_iter (DataLoader): DataLoader for the data.\n",
        "            train (bool): Whether to train the model (True) or test (False).\n",
        "        \"\"\"\n",
        "        str_code: str = \"train\" if train else \"test\"\n",
        "        avg_loss: float = 0.0\n",
        "        i: int = 0\n",
        "\n",
        "        print(\"Number of batches in DataLoader:\", len(data_iter))\n",
        "\n",
        "        for batch in data_iter:\n",
        "\n",
        "            batch = {key: value.to(self.device) for key, value in batch.items()}\n",
        "\n",
        "            encoder_ids = batch['encoder_ids']\n",
        "            decoder_ids = batch['decoder_ids']\n",
        "            labels = batch['labels']\n",
        "            mask = batch['attention_mask']\n",
        "\n",
        "            lm_output = self.model.forward(encoder_ids, decoder_ids, mask)\n",
        "\n",
        "            loss = self.criterion(lm_output.transpose(1, 2), labels)\n",
        "\n",
        "            if train:\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optim.step_and_update_lr()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"avg_loss\": avg_loss / (i + 1),\n",
        "                \"loss\": loss.item()\n",
        "            }\n",
        "\n",
        "            if i % self.log_freq == 0:\n",
        "                output_str = \"Epoch: {}, Iteration: {}, Avg Loss: {:.4f}, Current Loss: {:.4f}\".format(\n",
        "                    post_fix['epoch'], post_fix['iter'], post_fix['avg_loss'], post_fix['loss']\n",
        "                )\n",
        "                print(output_str)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        print(\"Epoch %d, %s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter))\n",
        "\n",
        "\n",
        "    def save(self, epoch: int) -> str:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        output_path: str = self.save_path + \".ep%d\" % epoch\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.cpu().state_dict(),\n",
        "            'optimizer_state_dict': self.optim.state_dict(),\n",
        "        }, output_path)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(\"Epoch %d Model and Optimizer State Saved on:\" % epoch, output_path)\n",
        "        return output_path"
      ],
      "metadata": {
        "id": "W6ATqrvIouRU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor"
      ],
      "metadata": {
        "id": "ux_iKZdHo3Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import argparse\n",
        "#import numpy as np\n",
        "#import random as rd\n",
        "#from trainer import T5Trainer\n",
        "#from adafactor import AdaFactor\n",
        "#from dataset import CustomTextDataset\n",
        "#from torch.utils.data import DataLoader\n",
        "#from scheduled_optim import ScheduledOptim\n",
        "\n",
        "\n",
        "def set_seeds(config):\n",
        "    \"\"\"\n",
        "    Sets the seed for random number generators in Python's `random` module, NumPy, and PyTorch to ensure reproducible results.\n",
        "    If CUDA is available and specified in the configuration, it also sets the seed for CUDA's random number generator and\n",
        "    makes CUDA's operations deterministic.\n",
        "\n",
        "    This function is crucial for experiments where reproducibility is important, as it ensures that the model initialization,\n",
        "    data shuffling, and other operations that rely on random number generation can be replicated exactly.\n",
        "\n",
        "    Args:\n",
        "        config (Config): A configuration object containing at least a `seed` attribute and a `with_cuda` boolean indicating\n",
        "                         whether CUDA-specific seeds need to be set for reproducibility.\n",
        "    \"\"\"\n",
        "    rd.seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "\n",
        "    if torch.cuda.is_available() and config.with_cuda:\n",
        "        torch.cuda.manual_seed_all(config.seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def run(config):\n",
        "    \"\"\"\n",
        "    The main function that sets up the environment, loads the dataset, initializes the T5 model along with its optimizer and\n",
        "    scheduler, and then runs the training and testing loops according to the provided configuration.\n",
        "\n",
        "    This function serves as the entry point for training the T5 model, orchestrating the process from dataset preparation\n",
        "    to model training and evaluation. It leverages the `T5Trainer` class to abstract away the complexities of the training\n",
        "    and testing loops.\n",
        "\n",
        "    Args:\n",
        "        config (Config): A configuration object containing all necessary parameters to initialize datasets, the model,\n",
        "                         optimizer, scheduler, and other components of the training process. This includes dataset paths,\n",
        "                         model hyperparameters, training options, device configuration, etc.\n",
        "\n",
        "    Note:\n",
        "        This function is designed to be called directly from the command line or as part of a script. It reads the configuration,\n",
        "        prepares the datasets, sets the computational device, initializes the model and its components, and finally starts the\n",
        "        training process followed by testing, if a test dataset is provided.\n",
        "    \"\"\"\n",
        "    # Set random seeds\n",
        "    set_seeds(config)\n",
        "\n",
        "    print(\"Loading Train Dataset...\")\n",
        "\n",
        "    # Load training dataset\n",
        "    train_dataset = CustomTextDataset(config.data_dir, tokenizer= config.tokenizer_path, max_token_len = config.max_token_len)\n",
        "\n",
        "    # Load test dataset if provided\n",
        "    test_dataset = CustomTextDataset(config.data_dir, tokenizer= config.tokenizer_path, max_token_len = config.max_token_len) if config.test_dataset is not None else None\n",
        "\n",
        "    # Setup cuda device for T5 training\n",
        "    cuda_condition: bool = torch.cuda.is_available() and config.with_cuda\n",
        "    device: torch.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
        "\n",
        "    # Initialize T5Model\n",
        "    t5 = T5Model(config).to(device)\n",
        "\n",
        "    # Distributed GPU training if CUDA can detect more than 1 GPU\n",
        "    if config.with_cuda and torch.cuda.device_count() > 1:\n",
        "        print(\"Using %d GPUs for T5Model\" % torch.cuda.device_count())\n",
        "        t5: nn.DataParallel = nn.DataParallel(t5, device_ids=config.cuda_devices)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    \"\"\"To use a manual (external) learning rate schedule you should set scale_parameter=False and relative_step=False.\n",
        "      In T5 case, additional optimizer operations like gradient clipping should not be used alongside Adafactor. We also set warmup_init to False.\n",
        "      # https://discuss.huggingface.co/t/t5-finetuning-tips/684\n",
        "    \"\"\"\n",
        "    optim = Adafactor(t5.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=None)\n",
        "    optim_schedule = ScheduledOptim(optim, config.hidden_size, config.n_warmup_steps)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = config.batch_size\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size = batch_size, worker_init_fn=np.random.seed(config.seed), shuffle = True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size= batch_size, worker_init_fn=np.random.seed(config.seed)) if test_dataset is not None else None\n",
        "\n",
        "    # Initialize t5 trainer\n",
        "    trainer = T5Trainer(config, t5, optim_schedule, device, train_data_loader, test_data_loader)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(config.epochs):\n",
        "        # Train the model\n",
        "        trainer.train(epoch)\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save(epoch)\n",
        "\n",
        "        # Test the model if test data is available\n",
        "        if test_data_loader is not None:\n",
        "            trainer.test(epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load configuration\n",
        "    config = Config(\n",
        "        prop=0.15,\n",
        "        tokenizer_path='t5-base',\n",
        "        max_token_len= 768,\n",
        "        bidirectional= True,\n",
        "        num_buckets= 32,\n",
        "        max_distance= 128,\n",
        "        data_dir= 'dataset/train.txt',\n",
        "        hidden_size= 768,\n",
        "        vocab_size= 32000,\n",
        "        hidden_dropout_prob= 0.1,\n",
        "        num_heads= 12,\n",
        "        num_blocks= 8,\n",
        "        n_warmup_steps= 10000,\n",
        "        lr= 0.01,\n",
        "        betas= (0.9, 0.999),\n",
        "        cuda_devices=None,\n",
        "        with_cuda= True,\n",
        "        log_freq= 10,\n",
        "        batch_size= 16,\n",
        "        save_path= 'tmp',\n",
        "        seed= 2024,\n",
        "        test_dataset= None,\n",
        "        epochs= 2\n",
        "    )\n",
        "\n",
        "    run(config)\n"
      ],
      "metadata": {
        "id": "GByQCqbgo5V1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}